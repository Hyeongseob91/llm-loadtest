# LLM Loadtest

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14-black?logo=next.js)](https://nextjs.org/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)

<p align="center">
  <a href="#english">English</a> â€¢
  <a href="#í•œêµ­ì–´">í•œêµ­ì–´</a>
</p>

---

<h2 id="english">English</h2>

> Load testing tool for LLM inference servers with real-time dashboard and AI-powered analysis

Comprehensive benchmarking system for vLLM, SGLang, Ollama, and other OpenAI-compatible API servers. Monitor performance in real-time and visualize results through an interactive web dashboard.

### Key Features

| Feature | Description |
|---------|-------------|
| **Accurate Metrics** | tiktoken-based token counting, LLM-specific metrics (TTFT, TPOT, ITL) |
| **Quality-Based Evaluation** | Goodput - measures the percentage of requests meeting SLO thresholds |
| **Real-time Monitoring** | WebSocket progress updates, GPU metrics (memory, utilization, temperature, power) |
| **Visualization** | Interactive charts, export to CSV/Excel |
| **Extensibility** | Adapter pattern supporting vLLM, SGLang, Ollama, Triton, and more |
| **AI Analysis** | LLM-powered benchmark analysis reports with Thinking model support |

---

### Quick Start

#### Docker Compose (Recommended)

```bash
# Clone the repository
git clone https://github.com/Hyeongseob91/llm-loadtest.git
cd llm-loadtest

# Start all services
docker compose up -d

# Access
# - Web UI: http://localhost:5050
# - API Docs: http://localhost:8085/docs
```

#### CLI Installation

```bash
# From project root
pip install -e .

# Basic load test
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --concurrency 1,10,50 \
  --num-prompts 100

# Goodput measurement (SLO-based)
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --concurrency 50 \
  --goodput ttft:500,tpot:50
```

---

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Web Dashboard                          â”‚
â”‚                    (Next.js + React)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ REST API / WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API Server                            â”‚
â”‚                       (FastAPI)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Shared Core                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Load         â”‚  â”‚ Metrics      â”‚  â”‚ GPU          â”‚      â”‚
â”‚  â”‚ Generator    â”‚  â”‚ Calculator   â”‚  â”‚ Monitor      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Adapters     â”‚  â”‚ Database     â”‚  â”‚ Validator    â”‚      â”‚
â”‚  â”‚ (vLLM, etc.) â”‚  â”‚ (SQLite)     â”‚  â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure (MSA)

```
llm-loadtest/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api/              # FastAPI backend server
â”‚   â”‚   â””â”€â”€ routers/      # API endpoints (benchmarks, websocket, recommend)
â”‚   â”œâ”€â”€ cli/              # Typer CLI tool
â”‚   â”‚   â””â”€â”€ commands/     # CLI commands (run, recommend, info, gpu)
â”‚   â””â”€â”€ web/              # Next.js dashboard
â”‚       â”œâ”€â”€ app/          # Pages (dashboard, benchmark, history)
â”‚       â””â”€â”€ components/   # UI components
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ core/             # Core logic
â”‚   â”‚   â”œâ”€â”€ load_generator.py   # Load generation engine
â”‚   â”‚   â”œâ”€â”€ metrics.py          # Metrics calculation
â”‚   â”‚   â”œâ”€â”€ gpu_monitor.py      # GPU monitoring
â”‚   â”‚   â”œâ”€â”€ validator.py        # Metrics validation
â”‚   â”‚   â””â”€â”€ models.py           # Data models
â”‚   â”œâ”€â”€ adapters/         # Server adapters
â”‚   â”‚   â”œâ”€â”€ base.py             # Adapter interface + factory
â”‚   â”‚   â””â”€â”€ openai_compat.py    # OpenAI API compatible adapter
â”‚   â””â”€â”€ database/         # SQLite storage
â”œâ”€â”€ docs/guides/          # Documentation
â””â”€â”€ docker-compose.yml
```

---

### Metrics

#### LLM-Specific Metrics

| Metric | Description | Unit | Calculation |
|--------|-------------|------|-------------|
| **TTFT** | Time To First Token | ms | First token arrival time - request start time |
| **TPOT** | Time Per Output Token | ms | (E2E - TTFT) / output token count |
| **E2E** | End-to-End Latency | ms | Response complete time - request start time |
| **ITL** | Inter-Token Latency | ms | Time interval between consecutive tokens |
| **Throughput** | Processing rate | tok/s | Total output tokens / test duration |
| **Request Rate** | Request processing rate | req/s | Completed requests / test duration |
| **Error Rate** | Error percentage | % | Failed requests / total requests Ã— 100 |

#### Goodput (Quality-Based Throughput)

Percentage of requests meeting all SLO (Service Level Objective) thresholds.

```bash
# Goodput measurement example
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --goodput ttft:500,tpot:50,e2e:5000
```

**Calculation:**
```
Goodput = (Requests where TTFT â‰¤ 500ms AND TPOT â‰¤ 50ms AND E2E â‰¤ 5000ms) / Total requests Ã— 100
```

#### Statistics

Each metric provides the following statistics:
- **min / max**: Minimum/Maximum values
- **mean**: Average
- **median (p50)**: Median value
- **p95 / p99**: Percentiles
- **std**: Standard deviation

---

### Supported Servers

| Server | Adapter | Status |
|--------|---------|--------|
| vLLM | openai | âœ… Supported |
| SGLang | openai | âœ… Supported |
| Ollama | openai | âœ… Supported |
| LMDeploy | openai | âœ… Supported |
| Triton | triton | ğŸš§ In Development |

Any server providing OpenAI-compatible API (`/v1/chat/completions`) is generally supported.

---

### CLI Commands

```bash
# Load test
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --concurrency 1,10,50,100 \
  --num-prompts 100 \
  --input-len 256 \
  --output-len 128 \
  --goodput ttft:500,tpot:50 \
  --stream

# Infrastructure recommendation
llm-loadtest recommend \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --peak-concurrency 500 \
  --ttft-target 500 \
  --goodput-target 95

# System info
llm-loadtest info

# GPU status
llm-loadtest gpu
```

---

### API Endpoints

**Base URL:** `http://localhost:8085/api/v1`

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/benchmark/run` | Start benchmark |
| `GET` | `/benchmark/{run_id}` | Get details |
| `GET` | `/benchmark` | List benchmarks |
| `DELETE` | `/benchmark/{run_id}` | Delete benchmark |
| `POST` | `/benchmark/{run_id}/cancel` | Cancel benchmark |
| `GET` | `/benchmark/{run_id}/export/csv` | Export to CSV |
| `GET` | `/benchmark/{run_id}/export/excel` | Export to Excel |
| `GET` | `/benchmark/result/{run_id}/analysis` | AI analysis report |
| `WS` | `/benchmark/{run_id}/progress` | Real-time progress |

---

### Web UI

| Page | Path | Features |
|------|------|----------|
| **Dashboard** | `/` | Benchmark list, recent runs |
| **New Benchmark** | `/benchmark/new` | Configure and start benchmarks |
| **Detail Page** | `/benchmark/[id]` | Real-time monitoring, results, AI analysis |
| **History** | `/history` | Past benchmark records |
| **Recommend** | `/recommend` | GPU infrastructure recommendation (Coming Soon) |
| **Compare** | `/compare` | Benchmark comparison (Coming Soon) |

---

### Tech Stack

**Backend:** Python 3.11+, FastAPI, asyncio/aiohttp, WebSocket, SQLite, tiktoken, pynvml, Typer

**Frontend:** Next.js 14, TypeScript, TanStack Query, Recharts, Tailwind CSS

**Infrastructure:** Docker Compose

---

### License

MIT License - See [LICENSE](LICENSE) for details.

---

### Contributing

Bug reports, feature requests, and PRs are welcome!

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

---

<h2 id="í•œêµ­ì–´">í•œêµ­ì–´</h2>

<details>
<summary><b>ğŸ“– í•œêµ­ì–´ ë¬¸ì„œ í¼ì¹˜ê¸°</b></summary>

> vLLM, SGLang, Ollama ë“± LLM ì„œë¹™ ì„œë²„ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ìµœì í™”í•˜ê¸° ìœ„í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

OpenAI-compatible API ì„œë²„ì˜ ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ Web ëŒ€ì‹œë³´ë“œì—ì„œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### í•µì‹¬ ê°€ì¹˜

| ê°€ì¹˜ | ì„¤ëª… |
|------|------|
| **ì •í™•í•œ ì¸¡ì •** | tiktoken ê¸°ë°˜ í† í° ì¹´ìš´íŒ…, LLM íŠ¹í™” ë©”íŠ¸ë¦­ (TTFT, TPOT, ITL) |
| **í’ˆì§ˆ ê¸°ë°˜ í‰ê°€** | Goodput - SLOë¥¼ ë§Œì¡±í•˜ëŠ” ìš”ì²­ ë¹„ìœ¨ ì¸¡ì • |
| **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§** | WebSocket ì§„í–‰ë¥ , GPU ë©”íŠ¸ë¦­ (ë©”ëª¨ë¦¬, í™œìš©ë¥ , ì˜¨ë„, ì „ë ¥) |
| **ì‹œê°í™”** | ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸, ê²°ê³¼ ë‚´ë³´ë‚´ê¸° (CSV/Excel) |
| **í™•ì¥ì„±** | ì–´ëŒ‘í„° íŒ¨í„´ìœ¼ë¡œ vLLM, SGLang, Ollama, Triton ë“± ì§€ì› |
| **AI ë¶„ì„** | LLM ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ë³´ê³ ì„œ, Thinking ëª¨ë¸ ì§€ì› |

---

### ë¹ ë¥¸ ì‹œì‘

#### Docker Compose (ê¶Œì¥)

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Hyeongseob91/llm-loadtest.git
cd llm-loadtest

# ì „ì²´ ì„œë¹„ìŠ¤ ì‹œì‘
docker compose up -d

# ì ‘ì†
# - Web UI: http://localhost:5050
# - API Docs: http://localhost:8085/docs
```

#### CLI ì„¤ì¹˜

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ
pip install -e .

# ê¸°ë³¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --concurrency 1,10,50 \
  --num-prompts 100

# Goodput ì¸¡ì • (SLO ê¸°ë°˜)
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --concurrency 50 \
  --goodput ttft:500,tpot:50
```

---

### ë©”íŠ¸ë¦­

#### LLM íŠ¹í™” ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ë‹¨ìœ„ | ê³„ì‚° ë°©ì‹ |
|--------|------|------|-----------|
| **TTFT** | Time To First Token | ms | ì²« í† í° ë„ì°© ì‹œê°„ - ìš”ì²­ ì‹œì‘ ì‹œê°„ |
| **TPOT** | Time Per Output Token | ms | (E2E - TTFT) / ì¶œë ¥ í† í° ìˆ˜ |
| **E2E** | End-to-End Latency | ms | ì‘ë‹µ ì™„ë£Œ ì‹œê°„ - ìš”ì²­ ì‹œì‘ ì‹œê°„ |
| **ITL** | Inter-Token Latency | ms | ì—°ì†ëœ í† í° ê°„ì˜ ì‹œê°„ ê°„ê²© |
| **Throughput** | ì²˜ë¦¬ëŸ‰ | tok/s | ì´ ì¶œë ¥ í† í° / í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ |
| **Request Rate** | ìš”ì²­ ì²˜ë¦¬ìœ¨ | req/s | ì™„ë£Œëœ ìš”ì²­ / í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ |
| **Error Rate** | ì˜¤ë¥˜ìœ¨ | % | ì‹¤íŒ¨ ìš”ì²­ / ì „ì²´ ìš”ì²­ Ã— 100 |

#### Goodput (í’ˆì§ˆ ê¸°ë°˜ ì²˜ë¦¬ëŸ‰)

SLO(Service Level Objective) ì„ê³„ê°’ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ìš”ì²­ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.

```bash
# Goodput ì¸¡ì • ì˜ˆì‹œ
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --goodput ttft:500,tpot:50,e2e:5000
```

**ê³„ì‚° ë°©ì‹:**
```
Goodput = (TTFT â‰¤ 500ms AND TPOT â‰¤ 50ms AND E2E â‰¤ 5000msì¸ ìš”ì²­ ìˆ˜) / ì „ì²´ ìš”ì²­ ìˆ˜ Ã— 100
```

---

### CLI ëª…ë ¹ì–´

```bash
# ë¶€í•˜ í…ŒìŠ¤íŠ¸
llm-loadtest run \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --concurrency 1,10,50,100 \
  --num-prompts 100 \
  --input-len 256 \
  --output-len 128 \
  --goodput ttft:500,tpot:50 \
  --stream

# ì¸í”„ë¼ ì¶”ì²œ
llm-loadtest recommend \
  --server http://localhost:8000 \
  --model qwen3-14b \
  --peak-concurrency 500 \
  --ttft-target 500 \
  --goodput-target 95

# ì‹œìŠ¤í…œ ì •ë³´
llm-loadtest info

# GPU ìƒíƒœ
llm-loadtest gpu
```

---

### API ì—”ë“œí¬ì¸íŠ¸

**Base URL:** `http://localhost:8085/api/v1`

| Method | Endpoint | ì„¤ëª… |
|--------|----------|------|
| `POST` | `/benchmark/run` | ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ |
| `GET` | `/benchmark/{run_id}` | ìƒì„¸ ì¡°íšŒ |
| `GET` | `/benchmark` | ëª©ë¡ ì¡°íšŒ |
| `DELETE` | `/benchmark/{run_id}` | ì‚­ì œ |
| `POST` | `/benchmark/{run_id}/cancel` | ì·¨ì†Œ |
| `GET` | `/benchmark/{run_id}/export/csv` | CSV ë‹¤ìš´ë¡œë“œ |
| `GET` | `/benchmark/{run_id}/export/excel` | Excel ë‹¤ìš´ë¡œë“œ |
| `GET` | `/benchmark/result/{run_id}/analysis` | AI ë¶„ì„ ë³´ê³ ì„œ |
| `WS` | `/benchmark/{run_id}/progress` | ì‹¤ì‹œê°„ ì§„í–‰ë¥  |

---

### Web UI

| í˜ì´ì§€ | ê²½ë¡œ | ê¸°ëŠ¥ |
|--------|------|------|
| **ëŒ€ì‹œë³´ë“œ** | `/` | ë²¤ì¹˜ë§ˆí¬ ëª©ë¡, ìµœê·¼ ì‹¤í–‰ ìƒíƒœ |
| **ìƒˆ ë²¤ì¹˜ë§ˆí¬** | `/benchmark/new` | ë²¤ì¹˜ë§ˆí¬ ì„¤ì • ë° ì‹œì‘ |
| **ìƒì„¸ í˜ì´ì§€** | `/benchmark/[id]` | ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§, ê²°ê³¼ ë¶„ì„, AI ë¶„ì„ ë³´ê³ ì„œ |
| **íˆìŠ¤í† ë¦¬** | `/history` | ê³¼ê±° ë²¤ì¹˜ë§ˆí¬ ì¡°íšŒ |
| **ì¸í”„ë¼ ì¶”ì²œ** | `/recommend` | GPU ê·œëª¨ ì¶”ì²œ (ì¤€ë¹„ì¤‘) |
| **ë¹„êµ** | `/compare` | ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (ì¤€ë¹„ì¤‘) |

---

### ê¸°ìˆ  ìŠ¤íƒ

**Backend:** Python 3.11+, FastAPI, asyncio/aiohttp, WebSocket, SQLite, tiktoken, pynvml, Typer

**Frontend:** Next.js 14, TypeScript, TanStack Query, Recharts, Tailwind CSS

**Infrastructure:** Docker Compose

---

### ë¼ì´ì„ ìŠ¤

MIT License - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

### ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ì œì•ˆ, PRì„ í™˜ì˜í•©ë‹ˆë‹¤!

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

</details>
